{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">© Moon</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report: Evaluation of the Centrality Algorithm, PageRank Part 2\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we briefly demonstrated about PageRank algorithm and simplified PageRank algorithm theoretical and empirical complexity. PageRank algorithms are important members of centrality algorithms which rank vertices of a graph by measuring the direct influence of nodes based on proportional rank.\n",
    "\n",
    "We empirically showed that the algorithm can run in O(N^2 * I) time where N represents total number of nodes given graph and I represents iterations.\n",
    "\n",
    "\n",
    "In this notebook, we will dive deeper. We will demonstrate how to use PR algorithm in real industry.\n",
    "\n",
    "This notebook demonstates:\n",
    "    - PR aglrotihm and adjusting limitations\n",
    "    - PR Implementation on Social media\n",
    "    - Topic-Specific (Personalized) PageRank\n",
    "    - Web Spam Detection Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement PR in Web, we need to consider several things in order to implement.\n",
    "\n",
    "Finding relevant and trusted documents in Web, are challenging.\n",
    "\n",
    "Challenges\n",
    "1. Finding trust sources:\n",
    "    1.  trusted pages have point to each otehr\n",
    "2. Finding \"best\" ranks\n",
    "3. all web pages are not equally \"important\"; requires weight\n",
    "4. There are a large diversity in the web-graph node connectivity; We need to rank pages by the link structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If a page j with importance rj has n out-links, each links receives rj/n scores.\n",
    "\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PageRank Algorithm\n",
    "\n",
    "The PageRank algorithm gives each page a rating of its\n",
    "importance, which is a recursively defined measure whereby a\n",
    "page becomes important if important pages link to it. \n",
    "\n",
    "The page rank of any page is the probability that the random surfer\n",
    "will land on a particular page that the surfer is more likely to end up in important pages.\n",
    "\n",
    "The behavior of the random surfer is an example of a Markov\n",
    "process, which depends only of the current state of a system.  \n",
    "The algorithm moves moves from state to state, based on probability distribution of the likelihood of moving from each state to every other possible state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Concepts\n",
    "1. Start with a set of pages. \n",
    "2. Crawl the web to determine the link structure. \n",
    "3. Assign each page an initial rank of 1 / N. \n",
    "4. update the rank of each page by adding up the\n",
    "weight of every page that links to it divided by the number\n",
    "of links emanating from the referring page.\n",
    "5. If a page has no outwardlinks, redistribute its rank equally among the other pages in\n",
    "the graph. \n",
    "6. Apply this redistribution to every page in the graph. \n",
    "7. Repeat this process until the page ranks stabilize. \n",
    "8. In practice, the Page Rank algorithm adds a damping factor\n",
    "at each stage to model the fact that users stop searching. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Steps\n",
    "The implementation of the classic PageRank algorithm uses an iterative method. At each iteration step, the PageRank value of all nodes in the graph are computed.\n",
    "\n",
    "1. Initialize the PageRank of every node with a value of 1/n\n",
    "2. Iterate through the graph. For each iteration, update the PageRank of every node in the graph.\n",
    "   1. For the first page, it only processes through random walk. \n",
    "   2. For other pages, they can process through random walk or inter-page links. \n",
    "   3. Sum up the proportional rank from all of its in-neighbors\n",
    "   4. Update the PageRank with the weighted sum of proportional rank and random walk\n",
    "3. Normalize the PageRank when there is terminal point. PageRank value will converge after enough iterations\n",
    "5. Return PR scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of the early PageRank \n",
    "\n",
    "In the early PageRank, there limitations:\n",
    "- [5]Rank Sinks: A rank sink occurs when a page does not link out. Rank sinks occurs when by refusing to share. \n",
    "\n",
    "- [5]Hoarding: a group of pages that only link between each other will also monopolize PageRank, creating error. \n",
    "\n",
    "- [5]Circular references: A couple of pages that only link between themselves and do not link to any other page. The iterative process will never converge, creating infinity loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustment for PR\n",
    "\n",
    "### First adjustment: Stochasticity Adjustment\n",
    "The PageRank equation computation requires summations which takes more computation time. To save the time, we can uses matrices to convert summations n to simpler vector-matrix multiplication, which saves computation time. \n",
    "\n",
    "Matrices also take advantage of matrix algebra and Markov Chains theory. \n",
    "The value (0 or 1) indicates whether or not there is a link between the pages. Instead of using 1 to indicate a link, we use 1/x, where x is the number of non-zero elements in each row. This strategy turns the non-zero values into probabilities, and creates a row substochastic matrix. Basically, this means that when you add the values of each row, some of the totals will equal 1 and the rest will equal zero. The zero totals happen because of the dangling nodes or rank sinks. For a row stochastic matrix all the rows must add up to 1.\n",
    "\n",
    "Leaving the matrix unmodified does not guarantee that the values will converge during iterations. In order to fix these problems, the first adjustment was introduced. It replaces all zero rows (dangling nodes/rank sinks) with 1/n eT (eT is a row vector of all 1s), making the matrix stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Second adjustment: Primitivity Adjustment\n",
    "Applying the Power Method to a Markov matrix converges to a unique as long as the matrix is stochastic, irreducible, and irregular.\n",
    "\n",
    "Intuitively, the primitive adjustment can be thought of as a random surfer that gets bored sometimes while following the hyperlink structure of the Web, and, instead of following links at random, enters a new URL in the browser navigation bar and continues from there. A proportion of the time he will be following links at random and a proportion of the time he will be 'teleporting' to a new URL.\n",
    "\n",
    "In order to model this mathematically, d (damping factor), it means that 85% of the time the surfer is following links at random, and 15% of the time he is entering new URLs in the browser bar.\n",
    "\n",
    "A new matrix is born from this adjustment. Let's call it G, the Google matrix.\n",
    "\n",
    "G = α S + (1 - α) 1/n eeT or G = α S + (1 - α) E, where E is the teleportation matrix. E = 1/n eeT (remember that eT is a row vector of all 1s)\n",
    "\n",
    "The teleporting is random because the teleportation matrix E = 1/n eeT is uniform, which means that the random surfer is equally likely to jump to any page when he teleports.\n",
    "\n",
    "One of the challenges for the designers of any search engine is\n",
    "ensuring that a commercial interest can’t artificially increase its\n",
    "ranking by creating many others pages whose only purpose is to\n",
    "link to that company’s home page.\n",
    "\n",
    "• Adopting the PageRank algorithm makes it harder for authors to\n",
    "manipulate the system because the ranking of a page depends\n",
    "on the prestige of important pages that are typically outside the\n",
    "control of those who are seeking to game the system.\n",
    "\n",
    "• Preventing users from manipulating their own web rankings is\n",
    "an ongoing problem for all search engine companies. To help\n",
    "ensure that the rankings remain fair, Google must keep the\n",
    "details of the ranking algorithms secret and change them often\n",
    "enough to outwit the would-be saboteurs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption\n",
    "\n",
    "For each node take the difference in PR score between the current iteration and the last iteration, if this error falls below a certain point the graph has converged.\n",
    "\n",
    "Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved.\n",
    "\n",
    "[6]Convergence is achieved when the error rate for any vertex in the graph falls below a given threshold value. The error rate of a vertex comuted by difference between the “real” score of the vertex PR(Vi) and the score computed at iteration I, PR^I(Vi) . The error rate is approximated at PR^(I+1)(Vi)+ PR^(I)(Vi).\n",
    "\n",
    "\n",
    "The computation of PR has no issue, if disregard scales. As damping factor increases, the rate of convergence also increases.\n",
    "\n",
    "The PageRank algorithm was designed for directed graphs. For this study, we will be using only directed graphs generated from NetworkX library. We will use damping factor as 0.85 and number of iterations as 100.\n",
    "\n",
    "\n",
    "The PageRank algorithm was designed for directed graphs. There are several factors\n",
    "\n",
    "\n",
    "The output (Numpy matrix) represents the transition matrix that describes the Markov chain used in PageRank. For PageRank to converge to a unique solution that there must be exists a path between every pair of nodes in the graph. Otherwise, there is a risk of being invalidated PR rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \"\"\"Returns the PageRank of the nodes in the graph.\n",
    "\n",
    "    PageRank computes a ranking of the nodes in the graph G based on\n",
    "    the structure of the incoming links. It was originally designed as\n",
    "    an algorithm to rank web pages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : graph\n",
    "      A NetworkX graph.  Undirected graphs will be converted to a directed\n",
    "      graph with two directed edges for each undirected edge.\n",
    "\n",
    "    d : float, optional\n",
    "      Damping factor for PageRank, default=0.85.\n",
    "\n",
    "    personalization: dict, optional\n",
    "      a nodes personalization value will be zero.\n",
    "      By default, a uniform distribution is used.\n",
    "\n",
    "    max_iter : integer, optional\n",
    "      Maximum number of iterations in power method eigenvalue solver.\n",
    "\n",
    "    tol : float, optional\n",
    "      Error tolerance used to check convergence in power method solver.\n",
    "\n",
    "    weight : weights are set to 1.\n",
    "\n",
    "    dangling: dict, optional\n",
    "      The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n",
    "      any outedges. \n",
    "      The dict key is the node the outedge points to and the dict\n",
    "      value is the weight of that outedge. By default, dangling nodes are given\n",
    "      outedges according to the personalization vector (uniform if not\n",
    "      specified). This must be selected to result in an irreducible transition\n",
    "      matrix. It may be common to have the\n",
    "      dangling dict to be the same as the personalization dict.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pagerank : dictionary\n",
    "       Dictionary of nodes with PageRank as value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank Implementation on other graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned previous section, the eigenvector calculation is done by the power iteration method and has no guarantee of convergence. The iteration will stop after an error tolerance of len(G) * tol has been reached. If the number of iterations exceed max_iter, a networkx.exception.PowerIterationFailedConvergence exception is raised.\n",
    "\n",
    "The PageRank algorithm was designed for directed graphs and if the input graph is directed and will execute on undirected graphs by converting each edge in the directed graph to two edges.\n",
    "\n",
    "### Assumption\n",
    "- personalization is uniform distribution\n",
    "\n",
    "\n",
    "    nstart : dictionary, optional\n",
    "      Starting value of PageRank iteration for each node.\n",
    "\n",
    "weight:\n",
    "    edge data key to use as weight. Set to 1\n",
    "\n",
    "dangling: dict, optional\n",
    "   The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n",
    "      any outedges. The dict key is the node the outedge points to and the dict\n",
    "      value is the weight of that outedge. By default, dangling nodes are given\n",
    "      outedges according to the personalization vector (uniform if not\n",
    "      specified). This must be selected to result in an irreducible transition\n",
    "      matrix (see notes under google_matrix). It may be common to have the\n",
    "      dangling dict to be the same as the personalization dict.\n",
    "\n",
    "      The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n",
    "      any outedges. The dict key is the node the outedge points to and the dict\n",
    "      value is the weight of that outedge. By default, dangling nodes are given\n",
    "      outedges according to the personalization vector (uniform if not\n",
    "      specified). This must be selected to result in an irreducible transition\n",
    "      matrix (see notes under google_matrix). It may be common to have the\n",
    "      dangling dict to be the same as the personalization dict.\n",
    "\n",
    "\n",
    "  PowerIterationFailedConvergence\n",
    "      If the algorithm fails to converge to the specified tolerance\n",
    "      within the specified number of iterations of the power iteration\n",
    "      method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "| **Input Argument** | **Type** | **Comment**                                       | \n",
    "|--------------------|----------|---------------------------------------------------|\n",
    "| G                  | graph    | input graph; Undirected graphs will be converted to a directed graph with two directed edges for each undirected edge. |\n",
    "| n                  | int      | total number of nodes of given graph (G)          |     \n",
    "| d                  | float, optional    | damping factor; the probability of random walk; default=0.85.                                   |\n",
    "| I                  | int, optional    | the number of iteration; Maximum number of iterations in power method; default=100 eigenvalue solver.                       |\n",
    "| tol | float | Error tolerance used to check convergence in power method solver.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "import networkx as nx\n",
    "\n",
    "def pageRank_python(G, d=0.85, I=100, tol=1.0e-6, weight=\"weight\"):\n",
    "    # edge case:\n",
    "    if len(G) == 0:                 # O(1)\n",
    "            return {}\n",
    "\n",
    "    D = G.to_directed()             # O(n + m) graph conversions\n",
    "\n",
    "    # Create a copy in (right) stochastic form\n",
    "    W = nx.stochastic_graph(D, weight)                      # O(n^2)\n",
    "    # get total number nodes of graph\n",
    "    N = W.number_of_nodes()                         \n",
    "    \n",
    "    # Initialize the PageRank of every node with a value of 1/n | O(n) \n",
    "    '''\n",
    "    x => PR\n",
    "    '''\n",
    "    PR = dict.fromkeys(W, 1.0 / N)\n",
    "    \n",
    "    # Assign uniform personalization vector\n",
    "    p = dict.fromkeys(W, 1.0 / N)\n",
    "    \n",
    "    # Set dangling_weights to persolization vector\n",
    "    dangling_weights = p\n",
    "    dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]\n",
    "    \n",
    "    # power iteration: make up to I iterations\n",
    "    for _ in range(I):\n",
    "        PRlast = PR\n",
    "        PR = dict.fromkeys(PRlast.keys(), 0)\n",
    "        danglesum = d * sum(PRlast[n] for n in dangling_nodes)\n",
    "        for n in PR:\n",
    "            # this matriPR multiply looks odd because it is\n",
    "            # doing a left multiply PR^T=PRlast^T*W\n",
    "            for _, nbr, wt in W.edges(n, data=weight):\n",
    "                PR[nbr] += d * PRlast[n] * wt\n",
    "            PR[n] += danglesum * dangling_weights.get(n, 0) + (1.0 - d) * p.get(n, 0)\n",
    "        # check convergence, l1 norm\n",
    "        err = sum(abs(PR[n] - PRlast[n]) for n in PR)\n",
    "        if err < N * tol:\n",
    "            return PR\n",
    "    raise nx.PowerIterationFailedConvergence(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.17543839772251535, 1: 0.32456160227748465, 2: 0.32456160227748465, 3: 0.17543839772251535}\n",
      "{0: 0.08850807396280014, 1: 0.05741484049711007, 2: 0.06276686454603018, 3: 0.037212081536313786, 4: 0.020503977347501656, 5: 0.03381044255357728, 6: 0.03152901134345505, 7: 0.02646461867880611, 8: 0.03338155566846445, 9: 0.00946321956579996, 10: 0.020689016083505596, 11: 0.009785686547904307, 12: 0.011474872305945287, 13: 0.03347418708532241, 14: 0.012941600888556289, 15: 0.016376332623593663, 16: 0.016755401561857987, 17: 0.009677265915396801, 18: 0.009544864590131916, 19: 0.01307751843108197, 20: 0.0112242350210376, 21: 0.011360152563563281, 22: 0.012960598606862793, 23: 0.041145969646022136, 24: 0.016634374450252683, 25: 0.028672962013730714, 26: 0.015240392773380827, 27: 0.027235358397633885, 28: 0.014478521774271624, 29: 0.02827181383282513, 30: 0.023031844250911867, 31: 0.04198548926127874, 32: 0.07592643687005647, 33: 0.09698041880501744}\n"
     ]
    }
   ],
   "source": [
    "G_path = nx.DiGraph(nx.path_graph(4))\n",
    "pr = pageRank_python(G)\n",
    "print(pr)\n",
    "\n",
    "G_social = nx.karate_club_graph()\n",
    "pr_social = pageRank_python(G_social)\n",
    "print(pr_social)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [8]path_graph: return path graphlinearly connected nodes.\n",
    "\n",
    "```\n",
    "G_path = nx.DiGraph(nx.path_graph(4))\n",
    "pr = pageRank_python(G)\n",
    "```\n",
    "\n",
    "- directed samples:\n",
    "    [9]scale_free_graph: Returns a scale-free directed graph.\n",
    "\n",
    "- undirected samples:\n",
    "  - [10]margulis_gabber_galil_graph\n",
    "  - ```\n",
    "returns the Margulis-Gabber-Galil undirected MultiGraph on n^2 nodes.\n",
    "\n",
    "The undirected MultiGraph is regular with degree 8. Nodes are integer pairs. The second-largest eigenvalue of the adjacency matrix of the graph is at most 5 sqrt{2}, regardless of n.```\n",
    "\n",
    "- other samples:\n",
    "  - Social networks\n",
    "    - [11]karate_club_graph(): Returns Zachary's Karate Club graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] A. Langville and C. Meyer,\n",
    "    \"A survey of eigenvector methods of web information retrieval.\"\n",
    "    http://citeseer.ist.psu.edu/713792.html\n",
    "[2] Page, Lawrence; Brin, Sergey; Motwani, Rajeev and Winograd, Terry,\n",
    "    The PageRank citation ranking: Bringing order to the Web. 1999\n",
    "    http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&doc=1999-66&format=pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "baa2e52980430d335b97b133b9bdc90c6ba1b985616caf224317f930da22a067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

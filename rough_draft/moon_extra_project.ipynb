{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">© Moon</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report: Evaluation of the Centrality Algorithm, PageRank Part 2\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we briefly demonstrated about PageRank algorithm and simplified PageRank algorithm theoretical and empirical complexity. PageRank algorithms are important members of centrality algorithms which rank vertices of a graph by measuring the direct influence of nodes based on proportional rank.\n",
    "\n",
    "We empirically showed that the algorithm can run in O(N^2 * I) time where N represents total number of nodes given graph and I represents iterations.\n",
    "\n",
    "\n",
    "In this notebook, we will dive deeper. We will demonstrate how to use PR algorithm in real industry.\n",
    "\n",
    "This notebook demonstates:\n",
    "    - PR aglrotihm and adjusting limitations\n",
    "    - PR Implementation on Social media\n",
    "    - Topic-Specific (Personalized) PageRank\n",
    "    - Web Spam Detection Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PageRank Algorithm\n",
    "\n",
    "The PageRank algorithm gives each page a rating of its\n",
    "importance, which is a recursively defined measure whereby a\n",
    "page becomes important if important pages link to it. \n",
    "\n",
    "The page rank of any page is the probability that the random surfer\n",
    "will land on a particular page that the surfer is more likely to end up in important pages.\n",
    "\n",
    "The behavior of the random surfer is an example of a Markov\n",
    "process, which depends only of the current state of a system.  \n",
    "The algorithm moves moves from state to state, based on probability distribution of the likelihood of moving from each state to every other possible state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Concepts\n",
    "1. Start with a set of pages. \n",
    "2. Crawl the web to determine the link structure. \n",
    "3. Assign each page an initial rank of 1 / N. \n",
    "4. update the rank of each page by adding up the\n",
    "weight of every page that links to it divided by the number\n",
    "of links emanating from the referring page.\n",
    "5. If a page has no outwardlinks, redistribute its rank equally among the other pages in\n",
    "the graph. \n",
    "6. Apply this redistribution to every page in the graph. \n",
    "7. Repeat this process until the page ranks stabilize. \n",
    "8. In practice, the Page Rank algorithm adds a damping factor\n",
    "at each stage to model the fact that users stop searching. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Steps\n",
    "The implementation of the classic PageRank algorithm uses an iterative method. At each iteration step, the PageRank value of all nodes in the graph are computed.\n",
    "\n",
    "1. Initialize the PageRank of every node with a value of 1/n\n",
    "2. Iterate through the graph. For each iteration, update the PageRank of every node in the graph.\n",
    "   1. For the first page, it only processes through random walk. \n",
    "   2. For other pages, they can process through random walk or inter-page links. \n",
    "   3. Sum up the proportional rank from all of its in-neighbors\n",
    "   4. Update the PageRank with the weighted sum of proportional rank and random walk\n",
    "3. Normalize the PageRank when there is terminal point. PageRank value will converge after enough iterations\n",
    "5. Return PR scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of the early PageRank \n",
    "\n",
    "In the early PageRank, there limitations:\n",
    "- [5]Rank Sinks: A rank sink occurs when a page does not link out. Rank sinks occurs when by refusing to share. \n",
    "\n",
    "- [5]Hoarding: a group of pages that only link between each other will also monopolize PageRank, creating error. \n",
    "\n",
    "- [5]Circular references: A couple of pages that only link between themselves and do not link to any other page. The iterative process will never converge, creating infinity loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustment for PR\n",
    "\n",
    "### First adjustment: Stochasticity Adjustment\n",
    "The PageRank equation computation requires summations which takes more computation time. To save the time, we can uses matrices to convert summations n to simpler vector-matrix multiplication, which saves computation time. \n",
    "\n",
    "Matrices also take advantage of matrix algebra and Markov Chains theory. \n",
    "\n",
    "In a matrix, the rows and columns are pages and the value (0 or 1) at the intersections indicates whether or not there is a link between the pages. Instead of using 1 to indicate a link, we use 1/x, where x is the number of non-zero elements in each row. This strategy turns the non-zero values into probabilities, and creates a row substochastic matrix. Basically, this means that when you add the values of each row, some of the totals will equal 1 and the rest will equal zero. The zero totals happen because of the dangling nodes or rank sinks. For a row stochastic matrix all the rows must add up to 1.\n",
    "\n",
    "In addition to the problems mentioned above, leaving the matrix unmodified does not guarantee that the values will ever converge, no matter how many iterations are performed. In order to fix these problems, the first adjustment was introduced. It replaces all zero rows (dangling nodes/rank sinks) with 1/n eT (eT is a row vector of all 1s), making the matrix stochastic. Let's call this modified matrix S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Second adjustment: Primitivity Adjustment\n",
    "\n",
    "In addition to solving the problems caused by rank sinks, it is desirable that the PageRank value of all pages is found quickly (in as few iterations as possible). Fortunately, applying the Power Method to a Markov matrix converges to a unique and positive vector called the stationary vector—in our case, the PageRank vector—as long as the matrix is stochastic, irreducible, and aperiodic. (Aperiodicity and irreducibility imply primitivity.)\n",
    "\n",
    "Intuitively, the primitive adjustment can be thought of as a random surfer that gets bored sometimes while following the hyperlink structure of the Web, and, instead of following links at random, enters a new URL in the browser navigation bar and continues from there. A proportion of the time he will be following links at random and a proportion of the time he will be 'teleporting' to a new URL.\n",
    "\n",
    "In order to model this mathematically, a new factor is introduced: α, a scalar between 0 and 1. Page and Brin originally defined α as 0.85. For this suggested α, it means that 85% of the time the surfer is following links at random, and 15% of the time he is entering new URLs in the browser bar.\n",
    "\n",
    "A new matrix is born from this adjustment. Let's call it G, the Google matrix.\n",
    "\n",
    "G = α S + (1 - α) 1/n eeT or G = α S + (1 - α) E, where E is the teleportation matrix. E = 1/n eeT (remember that eT is a row vector of all 1s)\n",
    "\n",
    "The teleporting is random because the teleportation matrix E = 1/n eeT is uniform, which means that the random surfer is equally likely to jump to any page when he teleports.\n",
    "\n",
    "One of the challenges for the designers of any search engine is\n",
    "ensuring that a commercial interest can’t artificially increase its\n",
    "ranking by creating many others pages whose only purpose is to\n",
    "link to that company’s home page.\n",
    "• Adopting the PageRank algorithm makes it harder for authors to\n",
    "manipulate the system because the ranking of a page depends\n",
    "on the prestige of important pages that are typically outside the\n",
    "control of those who are seeking to game the system.\n",
    "• Preventing users from manipulating their own web rankings is\n",
    "an ongoing problem for all search engine companies. To help\n",
    "ensure that the rankings remain fair, Google must keep the\n",
    "details of the ranking algorithms secret and change them often\n",
    "enough to outwit the would-be saboteurs.\n",
    "\n",
    "When you enter a set of search terms, Google allows you to\n",
    "search for a sequence of consecutive words by enclosing those\n",
    "words in quotation marks. In this example, searching for roast\n",
    "or mules is useless; searching for the quoted string \"roast mules\"\n",
    "brings the answer up immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption\n",
    "\n",
    "For each node take the difference in PR score between the current iteration and the last iteration, if this error falls below a certain point the graph has converged.\n",
    "\n",
    "Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved.\n",
    "\n",
    "[6]Convergence is achieved when the error rate for any vertex in the graph falls below a given threshold value. The error rate of a vertex comuted by difference between the “real” score of the vertex PR(Vi) and the score computed at iteration I, PR^I(Vi) . The error rate is approximated at PR^(I+1)(Vi)+ PR^(I)(Vi).\n",
    "\n",
    "\n",
    "The computation of PR has no issue, if disregard scales. As damping factor increases, the rate of convergence also increases.\n",
    "\n",
    "The PageRank algorithm was designed for directed graphs. For this study, we will be using only directed graphs generated from NetworkX library. We will use damping factor as 0.85 and number of iterations as 100.\n",
    "\n",
    "\n",
    "The PageRank algorithm was designed for directed graphs. There are several factors\n",
    "\n",
    "\n",
    "The output (Numpy matrix) represents the transition matrix that describes the Markov chain used in PageRank. For PageRank to converge to a unique solution that there must be exists a path between every pair of nodes in the graph. Otherwise, there is a risk of being invalidated PR rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \"\"\"Returns the PageRank of the nodes in the graph.\n",
    "\n",
    "    PageRank computes a ranking of the nodes in the graph G based on\n",
    "    the structure of the incoming links. It was originally designed as\n",
    "    an algorithm to rank web pages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : graph\n",
    "      A NetworkX graph.  Undirected graphs will be converted to a directed\n",
    "      graph with two directed edges for each undirected edge.\n",
    "\n",
    "    d : float, optional\n",
    "      Damping factor for PageRank, default=0.85.\n",
    "\n",
    "    personalization: dict, optional\n",
    "      a nodes personalization value will be zero.\n",
    "      By default, a uniform distribution is used.\n",
    "\n",
    "    max_iter : integer, optional\n",
    "      Maximum number of iterations in power method eigenvalue solver.\n",
    "\n",
    "    tol : float, optional\n",
    "      Error tolerance used to check convergence in power method solver.\n",
    "\n",
    "    weight : weights are set to 1.\n",
    "\n",
    "    dangling: dict, optional\n",
    "      The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n",
    "      any outedges. \n",
    "      The dict key is the node the outedge points to and the dict\n",
    "      value is the weight of that outedge. By default, dangling nodes are given\n",
    "      outedges according to the personalization vector (uniform if not\n",
    "      specified). This must be selected to result in an irreducible transition\n",
    "      matrix. It may be common to have the\n",
    "      dangling dict to be the same as the personalization dict.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pagerank : dictionary\n",
    "       Dictionary of nodes with PageRank as value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank Implementation on other graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageRank_graph(G, d=0.85, I=100, tol=1.0e-6):\n",
    "    if len(G) == 0:\n",
    "            return {}\n",
    "\n",
    "    D = G.to_directed()\n",
    "\n",
    "    # Create a copy in (right) stochastic form\n",
    "    W = nx.stochastic_graph(D)\n",
    "    # get total number nodes of graph\n",
    "    N = W.number_of_nodes()\n",
    "    \n",
    "    # Initialize the PageRank of every node with a value of 1/n | O(n) \n",
    "    '''\n",
    "    x => PR\n",
    "    '''\n",
    "    PR = dict.fromkeys(W, 1.0 / N)\n",
    "    \n",
    "    # Assign uniform personalization vector\n",
    "    p = dict.fromkeys(W, 1.0 / N)\n",
    "    \n",
    "    # Set dangling_weights to persolization vector\n",
    "    dangling_weights = p\n",
    "    dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]\n",
    "    \n",
    "    # power iteration: make up to I iterations\n",
    "    for _ in range(I):\n",
    "        PRlast = PR\n",
    "        PR = dict.fromkeys(PRlast.keys(), 0)\n",
    "        danglesum = d * sum(PRlast[n] for n in dangling_nodes)\n",
    "        for n in PR:\n",
    "            # this matriPR multiply looks odd because it is\n",
    "            # doing a left multiply PR^T=PRlast^T*W\n",
    "            for _, nbr, wt in W.edges(n):\n",
    "                PR[nbr] += d * PRlast[n] * wt\n",
    "            PR[n] += danglesum * \\\n",
    "                dangling_weights.get(n, 0) + (1.0 - d) * p.get(n, 0)\n",
    "        # check convergence, l1 norm\n",
    "        err = sum(abs(PR[n] - PRlast[n]) for n in PR)\n",
    "        if err < N * tol:\n",
    "            return PR\n",
    "    raise nx.PowerIterationFailedConvergence(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.17241401247723942,\n",
       " 1: 0.32758598752276064,\n",
       " 2: 0.32758598752276064,\n",
       " 3: 0.17241401247723942}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.DiGraph(nx.path_graph(4))\n",
    "pr = _pagerank_python(G, alpha=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] A. Langville and C. Meyer,\n",
    "    \"A survey of eigenvector methods of web information retrieval.\"\n",
    "    http://citeseer.ist.psu.edu/713792.html\n",
    "[2] Page, Lawrence; Brin, Sergey; Motwani, Rajeev and Winograd, Terry,\n",
    "    The PageRank citation ranking: Bringing order to the Web. 1999\n",
    "    http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&doc=1999-66&format=pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "baa2e52980430d335b97b133b9bdc90c6ba1b985616caf224317f930da22a067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

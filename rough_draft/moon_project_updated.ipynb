{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">Â© Moon</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report: Evaluation of the Centrality Algorithm, PageRank (Second Edition)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centrality algorithms are one of the categories of graph algorithms. They identify the important nodes in a given graph and those nodes are defined as vertices with many direct or indirect connections. One of the centrality algorithms is called $PageRank(PR)$. [1]Invented by Larry Page, PR is a link analysis algorithm used by Google Search to rank web pages in their search engine results. It ranks pages\n",
    "\n",
    " are important members of centrality algorithms which rank vertices of a graph by measuring the direct influence of nodes based on proportional rank. \n",
    "\n",
    "It is a link analysis algorithm and it assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web, with the purpose of \"measuring\" its relative importance within the set.\n",
    "\n",
    "In this notebook, we will demonstrated about the concepts of PageRank algorithm and its theoretical and empirical complexity.\n",
    "\n",
    "This notebook demonstrates:\n",
    "- The PageRank algorithm\n",
    "- Implementation the Classic PageRank algorithm and explore it on graphs generated from Networks, python library.\n",
    "- Measuring the PR complexity theoretically\n",
    "- Measuring the PR complexity empirically\n",
    "- PR aglrotihm and adjusting limitations\n",
    "- PR Implementation on Social media\n",
    "- Topic-Specific (Personalized) PageRank\n",
    "- Web Spam Detection Algorithms\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PageRank Algorithm\n",
    "\n",
    "The PageRank algorithm gives each page a rating of its importance, which is a recursively defined measure where by a page becomes important if important pages link to it. The page rank of any page is the probability that the random surfer will land on a particular page that the surfer is more likely to end up in important pages.\n",
    "\n",
    "The behavior of the random surfer is an example of a Markov process, which depends only of the current state of a system. The algorithm moves moves from state to state, based on probability distribution of the likelihood of moving from each state to every other possible state. \n",
    "\n",
    "\n",
    "\n",
    "[2]PageRank is a link analysis algorithm and it assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web, with the purpose of \"measuring\" its relative importance within the set. The output of PR is the probability that a person randomly surfing will arrive arrive at any particular page. It is assumed that the input distribution is evenly divided at the beginning of PR process. \n",
    "\n",
    "PR takes three inputs; $number of pages$, damping factor, and a number of iterations. The PageRank relies on an arbitrary probability distribution in which a person randomly clicks on links will arrive at any particular page. The probability which a person independently will continue is a damping factor $d$. PR computations require iterations through a number of pages to adjust approximate PR values to the theoretical value.\n",
    "\n",
    "[1]The result of node with a PR of 0.4 for instance, means there is 40% chance that a person randomly surf will be directed to the node. The implementation of the classic PageRank algorithm uses an iterative method. At each iteration step, the PageRank value of all nodes in the graph are computed.\n",
    "\n",
    "```\n",
    "There many PR algorithms.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement PR in Web, we need to consider several things in order to implement.\n",
    "\n",
    "Finding relevant and trusted documents in Web, are challenging.\n",
    "\n",
    "Challenges\n",
    "1. Finding trust sources:\n",
    "    1.  trusted pages have point to each otehr\n",
    "2. Finding \"best\" ranks\n",
    "3. all web pages are not equally \"important\"; requires weight\n",
    "4. There are a large diversity in the web-graph node connectivity; We need to rank pages by the link structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If a page j with importance rj has n out-links, each links receives rj/n scores.\n",
    "\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PageRank Algorithm\n",
    "\n",
    "The PageRank algorithm gives each page a rating of its\n",
    "importance, which is a recursively defined measure whereby a\n",
    "page becomes important if important pages link to it. \n",
    "\n",
    "The page rank of any page is the probability that the random surfer\n",
    "will land on a particular page that the surfer is more likely to end up in important pages.\n",
    "\n",
    "The behavior of the random surfer is an example of a Markov\n",
    "process, which depends only of the current state of a system.  \n",
    "The algorithm moves moves from state to state, based on probability distribution of the likelihood of moving from each state to every other possible state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Concepts\n",
    "1. Start with a set of pages. \n",
    "2. Crawl the web to determine the link structure. \n",
    "3. Assign each page an initial rank of 1 / N. \n",
    "4. update the rank of each page by adding up the\n",
    "weight of every page that links to it divided by the number\n",
    "of links emanating from the referring page.\n",
    "5. If a page has no outwardlinks, redistribute its rank equally among the other pages in\n",
    "the graph. \n",
    "6. Apply this redistribution to every page in the graph. \n",
    "7. Repeat this process until the page ranks stabilize. \n",
    "8. In practice, the Page Rank algorithm adds a damping factor\n",
    "at each stage to model the fact that users stop searching. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Steps\n",
    "The implementation of the classic PageRank algorithm uses an iterative method. At each iteration step, the PageRank value of all nodes in the graph are computed.\n",
    "\n",
    "1. Initialize the PageRank of every node with a value of 1/n\n",
    "2. Iterate through the graph. For each iteration, update the PageRank of every node in the graph.\n",
    "   1. For the first page, it only processes through random walk. \n",
    "   2. For other pages, they can process through random walk or inter-page links. \n",
    "   3. Sum up the proportional rank from all of its in-neighbors\n",
    "   4. Update the PageRank with the weighted sum of proportional rank and random walk\n",
    "3. Normalize the PageRank when there is terminal point. PageRank value will converge after enough iterations\n",
    "5. Return PR scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of the early PageRank \n",
    "\n",
    "In the early PageRank, there limitations:\n",
    "- [5]Rank Sinks: A rank sink occurs when a page does not link out. Rank sinks occurs when by refusing to share. \n",
    "\n",
    "- [5]Hoarding: a group of pages that only link between each other will also monopolize PageRank, creating error. \n",
    "\n",
    "- [5]Circular references: A couple of pages that only link between themselves and do not link to any other page. The iterative process will never converge, creating infinity loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustment for PR\n",
    "\n",
    "### First adjustment: Stochasticity Adjustment\n",
    "The PageRank equation computation requires summations which takes more computation time. To save the time, we can uses matrices to convert summations n to simpler vector-matrix multiplication, which saves computation time. \n",
    "\n",
    "Matrices also take advantage of matrix algebra and Markov Chains theory. \n",
    "The value (0 or 1) indicates whether or not there is a link between the pages. Instead of using 1 to indicate a link, we use 1/x, where x is the number of non-zero elements in each row. This strategy turns the non-zero values into probabilities, and creates a row substochastic matrix. Basically, this means that when you add the values of each row, some of the totals will equal 1 and the rest will equal zero. The zero totals happen because of the dangling nodes or rank sinks. For a row stochastic matrix all the rows must add up to 1.\n",
    "\n",
    "Leaving the matrix unmodified does not guarantee that the values will converge during iterations. In order to fix these problems, the first adjustment was introduced. It replaces all zero rows (dangling nodes/rank sinks) with 1/n eT (eT is a row vector of all 1s), making the matrix stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Second adjustment: Primitivity Adjustment\n",
    "Applying the Power Method to a Markov matrix converges to a unique as long as the matrix is stochastic, irreducible, and irregular.\n",
    "\n",
    "Intuitively, the primitive adjustment can be thought of as a random surfer that gets bored sometimes while following the hyperlink structure of the Web, and, instead of following links at random, enters a new URL in the browser navigation bar and continues from there. A proportion of the time he will be following links at random and a proportion of the time he will be 'teleporting' to a new URL.\n",
    "\n",
    "In order to model this mathematically, d (damping factor), it means that 85% of the time the surfer is following links at random, and 15% of the time he is entering new URLs in the browser bar.\n",
    "\n",
    "A new matrix is born from this adjustment. Let's call it G, the Google matrix.\n",
    "\n",
    "G = Î± S + (1 - Î±) 1/n eeT or G = Î± S + (1 - Î±) E, where E is the teleportation matrix. E = 1/n eeT (remember that eT is a row vector of all 1s)\n",
    "\n",
    "The teleporting is random because the teleportation matrix E = 1/n eeT is uniform, which means that the random surfer is equally likely to jump to any page when he teleports.\n",
    "\n",
    "One of the challenges for the designers of any search engine is\n",
    "ensuring that a commercial interest canât artificially increase its\n",
    "ranking by creating many others pages whose only purpose is to\n",
    "link to that companyâs home page.\n",
    "\n",
    "â¢ Adopting the PageRank algorithm makes it harder for authors to\n",
    "manipulate the system because the ranking of a page depends\n",
    "on the prestige of important pages that are typically outside the\n",
    "control of those who are seeking to game the system.\n",
    "\n",
    "â¢ Preventing users from manipulating their own web rankings is\n",
    "an ongoing problem for all search engine companies. To help\n",
    "ensure that the rankings remain fair, Google must keep the\n",
    "details of the ranking algorithms secret and change them often\n",
    "enough to outwit the would-be saboteurs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption\n",
    "\n",
    "For each node take the difference in PR score between the current iteration and the last iteration, if this error falls below a certain point the graph has converged.\n",
    "\n",
    "Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved.\n",
    "\n",
    "[6]Convergence is achieved when the error rate for any vertex in the graph falls below a given threshold value. The error rate of a vertex comuted by difference between the ârealâ score of the vertex PR(Vi) and the score computed at iteration I, PR^I(Vi) . The error rate is approximated at PR^(I+1)(Vi)+ PR^(I)(Vi).\n",
    "\n",
    "\n",
    "The computation of PR has no issue, if disregard scales. As damping factor increases, the rate of convergence also increases.\n",
    "\n",
    "The PageRank algorithm was designed for directed graphs. For this study, we will be using only directed graphs generated from NetworkX library. We will use damping factor as 0.85 and number of iterations as 100.\n",
    "\n",
    "\n",
    "The PageRank algorithm was designed for directed graphs. There are several factors\n",
    "\n",
    "\n",
    "The output (Numpy matrix) represents the transition matrix that describes the Markov chain used in PageRank. For PageRank to converge to a unique solution that there must be exists a path between every pair of nodes in the graph. Otherwise, there is a risk of being invalidated PR rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \"\"\"Returns the PageRank of the nodes in the graph.\n",
    "\n",
    "    PageRank computes a ranking of the nodes in the graph G based on\n",
    "    the structure of the incoming links. It was originally designed as\n",
    "    an algorithm to rank web pages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : graph\n",
    "      A NetworkX graph.  Undirected graphs will be converted to a directed\n",
    "      graph with two directed edges for each undirected edge.\n",
    "\n",
    "    d : float, optional\n",
    "      Damping factor for PageRank, default=0.85.\n",
    "\n",
    "    personalization: dict, optional\n",
    "      a nodes personalization value will be zero.\n",
    "      By default, a uniform distribution is used.\n",
    "\n",
    "    max_iter : integer, optional\n",
    "      Maximum number of iterations in power method eigenvalue solver.\n",
    "\n",
    "    tol : float, optional\n",
    "      Error tolerance used to check convergence in power method solver.\n",
    "\n",
    "    weight : weights are set to 1.\n",
    "\n",
    "    dangling: dict, optional\n",
    "      The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n",
    "      any outedges. \n",
    "      The dict key is the node the outedge points to and the dict\n",
    "      value is the weight of that outedge. By default, dangling nodes are given\n",
    "      outedges according to the personalization vector (uniform if not\n",
    "      specified). This must be selected to result in an irreducible transition\n",
    "      matrix. It may be common to have the\n",
    "      dangling dict to be the same as the personalization dict.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pagerank : dictionary\n",
    "       Dictionary of nodes with PageRank as value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank Implementation on other graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvector calculation is done by the power iteration method and has no guarantee of convergence. The iteration will stop after an error tolerance of len(G) * tol has been reached. If the number of iterations exceed max_iter, a networkx.exception.PowerIterationFailedConvergence exception is raised.\n",
    "\n",
    "The PageRank algorithm was designed for directed graphs and if the input graph is directed and will execute on undirected graphs by converting each edge in the directed graph to two edges.\n",
    "\n",
    "### Assumption\n",
    "- personalization is uniform distribution\n",
    "    nstart : dictionary, optional\n",
    "      Starting value of PageRank iteration for each node.\n",
    "\n",
    "weight:\n",
    "    edge data key to use as weight. Set to 1\n",
    "\n",
    "dangling: dict, optional\n",
    "   The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n",
    "      any outedges. The dict key is the node the outedge points to and the dict\n",
    "      value is the weight of that outedge. By default, dangling nodes are given\n",
    "      outedges according to the personalization vector (uniform if not\n",
    "      specified). This must be selected to result in an irreducible transition\n",
    "      matrix (see notes under google_matrix). It may be common to have the\n",
    "      dangling dict to be the same as the personalization dict.\n",
    "\n",
    "      The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n",
    "      any outedges. The dict key is the node the outedge points to and the dict\n",
    "      value is the weight of that outedge. By default, dangling nodes are given\n",
    "      outedges according to the personalization vector (uniform if not\n",
    "      specified). This must be selected to result in an irreducible transition\n",
    "      matrix (see notes under google_matrix). It may be common to have the\n",
    "      dangling dict to be the same as the personalization dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "| **Input Argument** | **Type** | **Comment**                                       | \n",
    "|--------------------|----------|---------------------------------------------------|\n",
    "| G                  | graph    | input graph; Undirected graphs will be converted to a directed graph with two directed edges for each undirected edge. |\n",
    "| n                  | int      | total number of nodes of given graph (G)          |     \n",
    "| d                  | float, optional    | damping factor; the probability of random walk; default=0.85.                                   |\n",
    "| I                  | int, optional    | the number of iteration; Maximum number of iterations in power method; default=100 eigenvalue solver.                       |\n",
    "| tol | float | Error tolerance used to check convergence in power method solver.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# data science packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# \n",
    "from warnings import warn\n",
    "import networkx as nx\n",
    "from time import perf_counter\n",
    "from random import randint\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageRank(G, d=0.85, I=100, tol=1.0e-6, weight=\"weight\"):\n",
    "    # edge case:\n",
    "    if len(G) == 0:                                         # O(1)\n",
    "            return {}\n",
    "\n",
    "    D = G.to_directed()                                     # O(n + m); graph conversions\n",
    "\n",
    "    # Create a copy in (right) stochastic form\n",
    "    W = nx.stochastic_graph(D, weight)                      # O(n^2); build a matrix and assign weight\n",
    "    # get total number nodes of graph\n",
    "    N = W.number_of_nodes()                                 # O(1); call stochastic_graph function which compute the number of nodes       \n",
    "    \n",
    "    # Initialize the PageRank of every node with a value of 1/n | O(n) \n",
    "    PR = dict.fromkeys(W, 1.0 / N)\n",
    "    \n",
    "    # Assign uniform personalization vector\n",
    "    p = dict.fromkeys(W, 1.0 / N)\n",
    "    \n",
    "    # Set dangling_weights to persolization vector\n",
    "    dangling_weights = p                                     # O(1); assign value      \n",
    "    dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]           # O(n) building array\n",
    "    \n",
    "    # power iteration: make up to I iterations                          # Total O(I * n) = O(I)    \n",
    "    for _ in range(I):\n",
    "        PRlast = PR                                                     # O(1) - assign value & computation\n",
    "        PR = dict.fromkeys(PRlast.keys(), 0)                            # O(1) - assign value & computation\n",
    "        danglesum = d * sum(PRlast[n] for n in dangling_nodes)          # O(1) - assign value & computation\n",
    "        \n",
    "        \n",
    "        for n in PR:                                                    # O(n) - assign value & computation\n",
    "            # this matriPR multiply looks odd because it is\n",
    "            # doing a left multiply PR^T=PRlast^T*W\n",
    "            for _, nbr, wt in W.edges(n, data=weight):                  # O(m)\n",
    "                PR[nbr] += d * PRlast[n] * wt                           # O(1) - assign value & computation\n",
    "            PR[n] += danglesum * dangling_weights.get(n, 0) + (1.0 - d) * p.get(n, 0)       # O(1) - assign value & computation\n",
    "        # check convergence, l1 norm\n",
    "        err = sum(abs(PR[n] - PRlast[n]) for n in PR)                   # O(1) - assign value & computation\n",
    "        if err < N * tol:                                               # O(1) - comparison\n",
    "            print('Convergence Test Passed!')                              # O(1) - assign value & computation\n",
    "            return PR                                                   # O(1) - assign value & computation\n",
    "    \n",
    "    print('convergence failed')                                         # O(1) - assign value & computation\n",
    "    # raise nx.PowerIterationFailedConvergence(I)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvector calculation is done by the power iteration method and has no guarantee of convergence. Our PR script (I=100, d = 0.85) include convergence test and will test our sample data by calling PR.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples Description Chart\n",
    "\n",
    "| **Notation** | **Type of Graph** | **Comment**                                       | \n",
    "|--------------------|----------|---------------------------------------------------|\n",
    "| G1                  | path graph    | [8]path graph linearly connected nodes.; nx.DiGraph(nx.path_graph(4)) |\n",
    "| G2                  | a scale-free directed graph      | [9]used in previous project; nx.scale_free_graph(n)\n",
    "| G3                  | undirected graph  | [10] returns the Margulis-Gabber-Galil undirected MultiGraph on n^2 nodes.; regular with degree 8. Nodes are integer pairs. The second-largest eigenvalue of the adjacency matrix of the graph is at most 5 sqrt{2}, regardless of n.                             |\n",
    "| G4                  | weighted graph   | Returns Zachary's Karate Club graph; has Each node in the returned graph has a node attribute 'club' tha t indicates the name of the club to which the member represented by that node belongs, either 'Mr. Hi' or 'Officer'. Each edge has a weight based on the number of contexts in which that edge's incident node members interacted.|  \n",
    "| G5 | biparitite graph | Returns Davis Southern women social network. This is a bipartite graph. | \n",
    "| G6 | weighted graph | florentine_families_graph()[source]: Returns Florentine families graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 4 nodes and 6 edges\n",
      "MultiDiGraph with 10 nodes and 13 edges\n",
      "MultiGraph named 'margulis_gabber_galil_graph(5)' with 25 nodes and 100 edges\n",
      "Graph named \"Zachary's Karate Club\" with 34 nodes and 78 edges\n",
      "Graph with 32 nodes and 89 edges\n",
      "Graph with 15 nodes and 20 edges\n"
     ]
    }
   ],
   "source": [
    "# Size of samples\n",
    "\n",
    "G1 = nx.DiGraph(nx.path_graph(4))\n",
    "G2 = nx.scale_free_graph(10)\n",
    "G3 = nx.margulis_gabber_galil_graph(5)\n",
    "G4 = nx.karate_club_graph()\n",
    "G5 = nx.davis_southern_women_graph()\n",
    "G6 = nx.florentine_families_graph()\n",
    "\n",
    "Samples = [G1, G2, G3, G4, G5, G6]\n",
    "\n",
    "# return graph sample's nodes & edges\n",
    "for G in Samples:\n",
    "    print(G) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Graph 1 Test Result:\n",
      "Convergence Test Passed!\n",
      "\n",
      "Sample Graph 2 Test Result:\n",
      "Convergence Test Passed!\n",
      "\n",
      "Sample Graph 3 Test Result:\n",
      "Convergence Test Passed!\n",
      "\n",
      "Sample Graph 4 Test Result:\n",
      "Convergence Test Passed!\n",
      "\n",
      "Sample Graph 5 Test Result:\n",
      "Convergence Test Passed!\n",
      "\n",
      "Sample Graph 6 Test Result:\n",
      "Convergence Test Passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convergence Test\n",
    "for num, G in enumerate(Samples, 1):\n",
    "    print('Sample Graph {num} Test Result:'.format(num=num))\n",
    "    pageRank(G)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the PageRank algorithm and the script in the previous cells. <br>\n",
    "The implementation of above PR algorithm uses an iterative method. At each iteration step, the PageRank value of all nodes in the graph are computed. <br>\n",
    "\n",
    "The time complexity depends on I, number of iterations and N, total number of nodes.\n",
    "\n",
    "The main loop runs I times, which is total number of iteration. <br>\n",
    "The inner loo p runs n times, which is the number of nodes generated <br>\n",
    "<br>\n",
    "\n",
    "The time complexityâs value is O(I * N) where I represents the specific number of iterations that needs to be run on node N. \n",
    "The space complexityâs value is O(N) where n is total number of nodes. since we keep only the given nodes information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Time Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empirical time analysis function takes number of nodes, pageRank function, and graph generation function.\n",
    "We will use the time.perf_counter to measure time and use a log scale plot to show small variations at a detailed level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[5]E. Guven, module05_ds, Jhu.edu. (2022).<br> https://jhu.instructure.com/courses/13110/pages/module-5-readings?module_item_id=1077894 (accessed July 24, 2022).<br>\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nx.number_of_nodes return total number of nodes of a graph\n",
    "\n",
    "def measure_cost(G, pageRank):\n",
    "    t = []\n",
    "    PR = None\n",
    "    num_nodes = nx.number_of_nodes(G)\n",
    "    \n",
    "    for n in range(num_nodes):\n",
    "        runs = []\n",
    "        st = perf_counter()\n",
    "        for i in range(n):\n",
    "            blockPrint()\n",
    "            PR = pageRank(G)\n",
    "            \n",
    "            runs += [perf_counter()-st]\n",
    "        t += [np.mean(runs)]\n",
    "\n",
    "    print('clock: ', ' '.join(['{:g}'.format(v) for v in t]))\n",
    "    # PR dataset can be used for search\n",
    "    return t, PR\n",
    "\n",
    "\n",
    "'''\n",
    "[5]E. Guven, module05_ds, Jhu.edu. (2022).<br> https://jhu.instructure.com/courses/13110/pages/module-5-readings?module_item_id=1077894 (accessed July 24, 2022).<br>\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blockPrint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/Moon/engineering/projects/pageRank_analysis/rough_draft/moon_project_updated.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Moon/engineering/projects/pageRank_analysis/rough_draft/moon_project_updated.ipynb#ch0000041?line=0'>1</a>\u001b[0m t1, ds1 \u001b[39m=\u001b[39m measure_cost(G1, pageRank)\n",
      "\u001b[1;32m/Users/Moon/engineering/projects/pageRank_analysis/rough_draft/moon_project_updated.ipynb Cell 41\u001b[0m in \u001b[0;36mmeasure_cost\u001b[0;34m(G, pageRank)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Moon/engineering/projects/pageRank_analysis/rough_draft/moon_project_updated.ipynb#ch0000041?line=9'>10</a>\u001b[0m st \u001b[39m=\u001b[39m perf_counter()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Moon/engineering/projects/pageRank_analysis/rough_draft/moon_project_updated.ipynb#ch0000041?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Moon/engineering/projects/pageRank_analysis/rough_draft/moon_project_updated.ipynb#ch0000041?line=11'>12</a>\u001b[0m     blockPrint()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Moon/engineering/projects/pageRank_analysis/rough_draft/moon_project_updated.ipynb#ch0000041?line=12'>13</a>\u001b[0m     PR \u001b[39m=\u001b[39m pageRank(G)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Moon/engineering/projects/pageRank_analysis/rough_draft/moon_project_updated.ipynb#ch0000041?line=14'>15</a>\u001b[0m     runs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [perf_counter()\u001b[39m-\u001b[39mst]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'blockPrint' is not defined"
     ]
    }
   ],
   "source": [
    "t1, ds1 = measure_cost(G1, pageRank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] A. Langville and C. Meyer,\n",
    "    \"A survey of eigenvector methods of web information retrieval.\"\n",
    "    http://citeseer.ist.psu.edu/713792.html\n",
    "[2] Page, Lawrence; Brin, Sergey; Motwani, Rajeev and Winograd, Terry,\n",
    "    The PageRank citation ranking: Bringing order to the Web. 1999\n",
    "    http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&doc=1999-66&format=pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "baa2e52980430d335b97b133b9bdc90c6ba1b985616caf224317f930da22a067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
